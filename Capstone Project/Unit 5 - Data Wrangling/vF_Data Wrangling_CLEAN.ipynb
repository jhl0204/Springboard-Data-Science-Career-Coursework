{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Top'></a>\n",
    "\n",
    "# Data Wrangling\n",
    "\n",
    "\n",
    "                                                   By: Jun Ho Lee\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Table of Contents'></a>\n",
    "## Table of Contents\n",
    "\n",
    "1. <a href='#Initial Importing Steps'>Initial Importing Steps</a>\n",
    "2. <a href='#Initial Inspection'>Initial Inspection</a>\n",
    "3. <a href='#Dataset Cleaning 1'>Dataset Cleaning 1 - Cleaning Columns</a>\n",
    "    1. <a href='#Dataset Cleaning 1'>Drop columns with missing values</a>\n",
    "    2. <a href='#Dataset Cleaning 1b.1'>Drop unnecessary columns - Agent/Attorney</a>\n",
    "    3. <a href='#Dataset Cleaning 1b.2'>Drop unnecessary columns - Prevailing Wage</a>\n",
    "    4. <a href='#Dataset Cleaning 1c'>Remove redundant columns - Area</a>\n",
    "4. <a href='#Numeric Columns'>Columns that should be numeric</a>\n",
    "5. <a href='#Dataset Cleaning 2'>Dataset Cleaning 2 - Cleaning Rows (Imputation)</a>\n",
    "    1. <a href='#Dataset Cleaning 2'>Imputing values for WAGE column</a>\n",
    "        1. <a href='#Prevailing Wage'>Note on PREVAILING-WAGE</a>\n",
    "    2. <a href='#Dataset Cleaning 2b'>Investigating Missing Values (H1B-DEPENDENT & WILLFUL-VIOLATOR columns)</a>\n",
    "    3. <a href='#Dataset Cleaning 2c'>Imputing values based on existing column relationship - STATE & POSTAL-CODE</a>\n",
    "    4. <a href='#Dataset Cleaning 2d'>POSTAL-CODES that are *\"weird\"*</a>\n",
    "    5. <a href='#Dataset Cleaning 2e'>Final Missing Value Handling</a>\n",
    "\n",
    "<a href='#Top'>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Initial Importing Steps'></a>\n",
    "____\n",
    "### 1. Initial Importing Steps\n",
    "\n",
    "Using the column information saved in the JSON file, we will load in the dataset so that memory usage efficiency can be improved. The initial loading step would be done for every subsequent new notebooks but the steps will be only detailed in this current ***DATA WRANGLING*** notebook. In future notebooks, the initial dataset importing step will be wrapped in a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**A. Import Necessary Libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# # set option to see all the columns\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Previous function to check memory usage of new dataframe** \n",
    "\n",
    "Adapted from [Dataquest](https://www.dataquest.io/blog/pandas-big-data/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj, pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Import JSON file and extract column information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import JSON file for column information extraction \n",
    "import json\n",
    "\n",
    "with open('data/h1b_df_params.json', 'r') as fp:\n",
    "    h1b_params = json.load(fp)\n",
    "\n",
    "final_cols = h1b_params['c_final']\n",
    "column_types = h1b_params['c_type']\n",
    "date_columns = h1b_params['c_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D. Read in the original dataframe with optimized column parameters** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't RUN this TWICE!! \n",
    "h1b_df = pd.read_csv(\"data/H-1B_Disclosure_Data_FY2018_EOY.csv\", usecols=final_cols, dtype=column_types, low_memory=False, parse_dates=date_columns,infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E. Check memory usage to verify optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204.63 MB\n",
      "Initial dimensinos of the dataframe: (654360, 50)\n"
     ]
    }
   ],
   "source": [
    "# Check Memory Usage \n",
    "print(mem_usage(h1b_df))\n",
    "print(\"Initial dimensinos of the dataframe: {}\".format(h1b_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#Table of Contents'>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Initial Inspection'></a>\n",
    "___\n",
    "### 2. Initial Inspection\n",
    "\n",
    "Now that the dataset is read in, let's take a look at the shape of the dataset. Initial inspection (`h1b_df.shape`) tells us that the current dataset has a total of 50 columns and 654360 rows. Let's dive in deeper with `.info()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Commented out due to space constraints\n",
    "# h1b_df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**From the initial inspection, three things stand out:**\n",
    "1. There are a number of columns that have a lot of missing values\n",
    "2. There are some columns that are probably not useful in predicting the outcome variable (`CASE_STATUS`). I would need to make a judgment call on this matter.\n",
    "3. There are a few columns whose information are redundant and can probably be explained by another existing variable (ex. `EMPLOYER_POSTAL_CODE` is a unique code that can explain `CITY`, `ADDRESS` information etc.) \n",
    "\n",
    "\n",
    "To clean our dataset and reduce colinearity among the features for future machine learning pipeline, we will identify these columns and drop them from the original dataframe. To identify columns with **'many'** missing values, I will set a threshold of 50% and drop those columns that are above the threshold. \n",
    "\n",
    "After cleaning the columns let's take another look at the dataset.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#Table of Contents'>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Dataset Cleaning 1'></a>\n",
    "___\n",
    "### 3. Dataset Cleaning 1 - Cleaning Columns\n",
    "\n",
    "### 3a. Drop columns with missing values\n",
    "- As a first step, I will be dropping columns that have more than 50% of their rows missing since those rows will not provide any useful information on predicting the outcome variable (`CASE_STATUS`)\n",
    "\n",
    "- I will be saving the new dataframe to `h1b_df_clean` and updating (dropping) the dataframe columns sequentially throughout this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Function to determine missing value percentage**\n",
    "\n",
    "- Modified from [Dametreusv Github](https://github.com/dametreusv/world_development_indicators_data_science/blob/master/WDI_wrangle.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modified from https://github.com/dametreusv/world_development_indicators_data_science/blob/master/WDI_wrangle.ipynb\n",
    "\n",
    "def find_cols_to_drop(df):\n",
    "    cols_to_drop = []\n",
    "    columns = df.columns\n",
    "    for column in columns:\n",
    "        percentage = (df[column].isnull().sum() / df.shape[0]) * 100\n",
    "    \n",
    "        # append columns to \"drop_list\" if the missing value percentage is greater than 50% \n",
    "        if percentage > 50:\n",
    "            cols_to_drop.append(column)\n",
    "        \n",
    "    return cols_to_drop\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Drop the columns that meet the above criteria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = find_cols_to_drop(h1b_df)\n",
    "h1b_df_clean = h1b_df.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Cleaned Dataframe Dimensions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(654360, 44)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1b_df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#Table of Contents'>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Dataset Cleaning 1b.1'></a>\n",
    "___\n",
    "### 3b. Drop unnecessary columns - Part 1 (Agent/Attorney)\n",
    "\n",
    "- For the second step, I will be making a judgment call based on domain knowledge. My end goal is to predict `CASE_STATUS` given our predictor variables. Given this end goal, I believe information on `AGENT_ATTORNEY` will not be useful at all with the exception of `AGENT_REPRESENTING_EMPLOYER`, which contains information on whether the employer was represented / not represented by an agent or an attorney. Therefore we will drop the columns that have information on `AGENT_ATTORNEY`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Selecting columns that contain 'Agent/Attorney'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting column names that contain 'AGENT ATTORNEY'\n",
    "cols = h1b_df_clean.columns\n",
    "attorney_bool = cols.str.contains(\"AGENT_ATTORNEY\")\n",
    "attorney_idx = np.where(attorney_bool)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Drop the columns that meet the above criteria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_agent_cols = cols[attorney_idx]\n",
    "h1b_df_clean2 = h1b_df_clean.drop(drop_agent_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Cleaned Dataframe Dimensions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(654360, 41)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1b_df_clean2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#Table of Contents'>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Dataset Cleaning 1b.2'></a>\n",
    "___\n",
    "### 3b. Drop unnecessary columns - Part 2 (Prevailing Wage)\n",
    "\n",
    "- The h1b dataframe also has a column called `PREVAILING WAGE` and other columns based on `PREVAILING WAGE`. Looking at the data dictionary however, it is difficult to understand what `PW_SOURCE` signifies. The variables for this column (\"OES\" / \"CBA\" / \"DBA\" / \"SCA\" / \"Other\") are not explained at all. Therefore I will drop all columns associated with `PW_SOURCE` since the variables would not be interpretable. \n",
    "\n",
    "- I will additionally drop the `PW_WAGE_LEVEL` column since I can create custom wage levels based on the `PREVAILING_WAGE` column. `PW_WAGE_LEVEL` information would be redundant. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Selecting columns that contain 'PW_Source' and 'PW_Wage_Level**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting column names that contain 'PW_SOURCE' and 'PW_WAGE_LEVEL' \n",
    "cols = h1b_df_clean2.columns\n",
    "pw_bool = cols.str.contains(\"PW_SOURCE|PW_WAGE_LEVEL\")\n",
    "pw_idx = np.where(pw_bool)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Drop the columns that meet the above criteria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_pw_cols = cols[pw_idx]\n",
    "h1b_df_clean3 = h1b_df_clean2.drop(drop_pw_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Cleaned Dataframe Dimensions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(654360, 37)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1b_df_clean3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#Table of Contents'>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Dataset Cleaning 1c'></a>\n",
    "___\n",
    "### 3c. Remove redundant columns\n",
    "\n",
    "- For the last step, I will also be making a judgment call. I know that the ***POSTAL_CODE*** can uniquely describe every area of a country and thus will describe *ADDRESS, CITY, PROVINCE, COUNTRY, STATE* information as well. Therefore we can solely retain the ***POSTAL_CODE*** column and still capture the entire area information. This comes at a cost however, since ***POSTAL_CODE*** information is very granular. Therefore I will still retain *STATE* information in the `cleaned dataset` to observe any trends in the analysis. Moreover, I will save all the area columns (including ***POSTAL_CODE*** column) into a separate dataframe so that I can query into the arae dataframe and come back later for investigation if necessary. The cleaned dataframe will have ***POSTAL_CODE*** and ***STATE*** columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Selecting columns that contain AREA information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # selecting column names that contain 'ADDRESS|COUNTY|CITY|PROVINCE|STATE|POSTAL_CODE|COUNTRY'\n",
    "# # Note that 'PROVINCE' column was deleted from 3A\n",
    "\n",
    "cols = h1b_df_clean3.columns\n",
    "area_bool = cols.str.contains(\"ADDRESS|COUNTY|CITY|PROVINCE|STATE|POSTAL_CODE|COUNTRY\")\n",
    "area_idx = np.where(area_bool)[0]\n",
    "area_cols = cols[area_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Create new `area dataframe` and save it as a csv file along with its dtypes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Previous function that returns column dtypes* \n",
    "- Needed later when reading in `area_df` from a different notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_col_dtypes(df):\n",
    "    dtypes = df.dtypes\n",
    "\n",
    "    col_name = dtypes.index                      # name of the column \n",
    "    col_types = [i.name for i in dtypes.values]  # datatype of the column\n",
    "\n",
    "    # # Save column name and types into a dictionary \n",
    "    # # {name: type}\n",
    "    column_types = dict(zip(col_name, col_types))\n",
    "    return column_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Saving the `area_df` to csv file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_cols = cols[area_idx]\n",
    "area_df = h1b_df_clean3[area_cols]\n",
    "area_df.to_csv(\"data/area_df\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Save column types into a json format*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_col_types = save_col_dtypes(area_df)\n",
    "\n",
    "# convert to json format\n",
    "with open('data/area_df_params.json', 'w') as fp:\n",
    "    json.dump(area_col_types, fp)  # convert dict as a json file format! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Drop the area columns except `STATE` and `POSTAL CODE` columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # selecting column names that contain 'ADDRESS|COUNTY|CITY|PROVINCE|COUNTRY'\n",
    "# # Note that 'PROVINCE' column was deleted from 3A\n",
    "\n",
    "cols = h1b_df_clean3.columns\n",
    "drop_area_bool = cols.str.contains(\"ADDRESS|COUNTY|CITY|PROVINCE|COUNTRY\")\n",
    "drop_area_idx = np.where(drop_area_bool)[0]\n",
    "drop_area_cols = cols[drop_area_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D. Drop the columns that meet the above criteria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1b_df_clean4 = h1b_df_clean3.drop(drop_area_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E. Cleaned Dataframe Dimensions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(654360, 32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1b_df_clean4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#Table of Contents'>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Numeric Columns'></a>\n",
    "____\n",
    "### 4. Columns that should be numeric\n",
    "\n",
    "Now that we cleaned up the columns of the dataframe, let's take another look at the data - this time thinking about whether the **dtypes** we picked for each column were actually valid.\n",
    "\n",
    "In fact, if we investigate the *dtypes* for columns `PREVAILING_WAGE`, `WAGE_RATE_OF_PAY_FROM`, `WAGE_RATE_OF_PAY_TO`, they are either *Categorical* or *Object* dtypes. This doesn't make sense since wage is always expressed in numbers and thus would have to be a numeric *dtype*(`int` or `float`). This suggests that some values in these columns have non-numeric characters such as commas(,) or periods(.). We will clean up these non-numeric characters and convert them to a numeric *dtype*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Remove non-numeric characters and convert to numeric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove commas (non_numeric char) and convert to numeric (float) dtype\n",
    "# # For three columns\n",
    "h1b_df_clean4.PREVAILING_WAGE = pd.to_numeric(h1b_df_clean4.PREVAILING_WAGE.str.replace(',',''), downcast='float')\n",
    "h1b_df_clean4.WAGE_RATE_OF_PAY_FROM = pd.to_numeric(h1b_df_clean4.WAGE_RATE_OF_PAY_FROM.str.replace(',',''),downcast='float')\n",
    "h1b_df_clean4.WAGE_RATE_OF_PAY_TO = pd.to_numeric(h1b_df_clean4.WAGE_RATE_OF_PAY_TO.str.replace(',',''),downcast='float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Check datatypes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Commented out due to space constraints\n",
    "# h1b_df_clean4.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#Table of Contents'>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Dataset Cleaning 2'></a>\n",
    "____\n",
    "### 5. Dataset Cleaning 2 - Cleaning Rows (Imputation)\n",
    "\n",
    "### 5a. Imputing `WAGE_RATE_OF_PAY_TO` Column\n",
    "\n",
    "Now that we cleaned up the columns of the dataframe and converted necessary columns to more suitable datatypes, let's take another look at the data - this time focusing on the rows. If there are any missing or unordinary values, we would need to impute and replace them with more suitable values. \n",
    "\n",
    "Let's actually first take a look at the first five rows of the three columns that we converted to numeric dtype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Initial Investigation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PREVAILING_WAGE</th>\n",
       "      <th>WAGE_RATE_OF_PAY_FROM</th>\n",
       "      <th>WAGE_RATE_OF_PAY_TO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112549.0</td>\n",
       "      <td>143915.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79976.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77792.0</td>\n",
       "      <td>78240.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84406.0</td>\n",
       "      <td>84406.0</td>\n",
       "      <td>85000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87714.0</td>\n",
       "      <td>95000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PREVAILING_WAGE  WAGE_RATE_OF_PAY_FROM  WAGE_RATE_OF_PAY_TO\n",
       "0         112549.0               143915.0                  0.0\n",
       "1          79976.0               100000.0                  0.0\n",
       "2          77792.0                78240.0                  0.0\n",
       "3          84406.0                84406.0              85000.0\n",
       "4          87714.0                95000.0                  0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_df = h1b_df_clean4.loc[:,['PREVAILING_WAGE', 'WAGE_RATE_OF_PAY_FROM', 'WAGE_RATE_OF_PAY_TO']]\n",
    "numeric_df.head()\n",
    "# numeric_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** Upon inspection, there are a lot of 0s in the  `WAGE_RATE_OF_PAY_TO` column. Let's actually see how many zeros there are in this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of zeros in WAGE_RATE_OF_PAY_TO column: 511240\n",
      "# of nan in WAGE_RATE_OF_PAY_TO column: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PREVAILING_WAGE</th>\n",
       "      <th>WAGE_RATE_OF_PAY_FROM</th>\n",
       "      <th>WAGE_RATE_OF_PAY_TO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>26.280001</td>\n",
       "      <td>26.280001</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31918</th>\n",
       "      <td>26.280001</td>\n",
       "      <td>26.280001</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34803</th>\n",
       "      <td>72842.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PREVAILING_WAGE  WAGE_RATE_OF_PAY_FROM  WAGE_RATE_OF_PAY_TO\n",
       "119          26.280001              26.280001                  NaN\n",
       "31918        26.280001              26.280001                  NaN\n",
       "34803     72842.000000           75000.000000                  NaN"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns the length (number) of zeros in this particular column\n",
    "n_of_zeros_in_column = len(np.where(numeric_df.WAGE_RATE_OF_PAY_TO == 0)[0])\n",
    "print(\"# of zeros in WAGE_RATE_OF_PAY_TO column: {}\".format(n_of_zeros_in_column))\n",
    "\n",
    "n_of_nan_in_column = numeric_df.WAGE_RATE_OF_PAY_TO.isnull().sum()\n",
    "print(\"# of nan in WAGE_RATE_OF_PAY_TO column: {}\".format(n_of_nan_in_column))\n",
    "\n",
    "to_null_idx = np.where(numeric_df.WAGE_RATE_OF_PAY_TO.isnull())\n",
    "numeric_df.loc[to_null_idx] # print out the dataframe  of only nan values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** It looks like majority of the values are zeros and only three values are `nan` values. Further inspection reveals that the rows at which `WAGE_RATE_OF_PAY_TO` (TO) are nan values do have values for `PREVAILING WAGE` and `WAGE_RATE_OF_PAY_FROM` (FROM). It looks like zeros were inputted as substitutes for `nan` values. Since the zeros can be essentially be thought as `nan` values, we would need to impute accordingly. \n",
    "\n",
    "The non-zero values of `TO` are typically greater than `FROM` values. Thus it would be logical to impute `TO` values based on `FROM` values. If `TO` values are zero or `nan` values, I will replace them with the values from the `FROM` column.\n",
    "\n",
    "Additionally, if any values from the `FROM` column are zeros, then that means those values are essentially `nan` values. I will drop those rows as a last step. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Imputing `WAGE_RATE_OF_PAY_TO` column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Remember to perform operation on the entire cleaned h1b dataset\n",
    "# # # NOT the numeric_df, which was just used for testing purposes. \n",
    "\n",
    "# # Remember that .loc returns a group of rows!\n",
    "# # If any values from `TO` column are zeros, replace with the value from `FROM` column\n",
    "h1b_df_clean4.loc[h1b_df_clean4['WAGE_RATE_OF_PAY_TO'] == 0, ['WAGE_RATE_OF_PAY_TO']] = h1b_df_clean4['WAGE_RATE_OF_PAY_FROM']\n",
    "\n",
    "\n",
    "# # If any values from `TO` column are nan, replace with the value from `FROM` column\n",
    "# # Can't use | operator with nans\n",
    "h1b_df_clean4.loc[np.isnan(h1b_df_clean4.WAGE_RATE_OF_PAY_TO), ['WAGE_RATE_OF_PAY_TO']] = h1b_df_clean4['WAGE_RATE_OF_PAY_FROM']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Drop rows where values of `FROM` are zeros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of zeros in WAGE_RATE_OF_PAY_FROM column: 12\n"
     ]
    }
   ],
   "source": [
    "# # First check the number of zeros in `FROM` column\n",
    "n_of_zeros_in_from = len(np.where(h1b_df_clean4.WAGE_RATE_OF_PAY_FROM == 0)[0])\n",
    "print(\"# of zeros in WAGE_RATE_OF_PAY_FROM column: {}\".format(n_of_zeros_in_from))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the index where the values are zero\n",
    "zero_idx = np.where(h1b_df_clean4.WAGE_RATE_OF_PAY_FROM == 0)[0]\n",
    "\n",
    "# # observe the dataframe\n",
    "h1b_df_clean4.loc[zero_idx]\n",
    "\n",
    "# # Drop the row indices (DON'T USE inplace operation!!) and set a new dataframe\n",
    "h1b_df_clean5 = h1b_df_clean4.drop(zero_idx)\n",
    "\n",
    "# # RESET the index so that the dropped indices don't affect future calculations\n",
    "# # VERY VERY IMPT!\n",
    "h1b_df_clean6 = h1b_df_clean5.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Resetting the index after dropping the rows is CRUCIAL. If you don't reset the index, the dataframe will retain the original index but internally assign a new index in future manipulations. Thus subsequent indexing will be errorneous!! \n",
    "\n",
    "Ex.)  \n",
    "DataFrame with index column = [0, 1, 2, 3, 4, 5] <br>\n",
    "Original Index --> 0, 1, 2, 3, 4, 5 <br>\n",
    "Dropped Index --> 3 <br>\n",
    "Retained Index --> 0, 1, 2,    4, 5 <br>\n",
    "Future Manipulation (index 3 --> returns 4!!) (since internally re-indexes from the start)\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASE_NUMBER</th>\n",
       "      <th>CASE_STATUS</th>\n",
       "      <th>CASE_SUBMITTED</th>\n",
       "      <th>DECISION_DATE</th>\n",
       "      <th>VISA_CLASS</th>\n",
       "      <th>EMPLOYMENT_START_DATE</th>\n",
       "      <th>EMPLOYMENT_END_DATE</th>\n",
       "      <th>EMPLOYER_NAME</th>\n",
       "      <th>EMPLOYER_STATE</th>\n",
       "      <th>EMPLOYER_POSTAL_CODE</th>\n",
       "      <th>AGENT_REPRESENTING_EMPLOYER</th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>SOC_CODE</th>\n",
       "      <th>SOC_NAME</th>\n",
       "      <th>NAICS_CODE</th>\n",
       "      <th>TOTAL_WORKERS</th>\n",
       "      <th>NEW_EMPLOYMENT</th>\n",
       "      <th>CONTINUED_EMPLOYMENT</th>\n",
       "      <th>CHANGE_PREVIOUS_EMPLOYMENT</th>\n",
       "      <th>NEW_CONCURRENT_EMP</th>\n",
       "      <th>CHANGE_EMPLOYER</th>\n",
       "      <th>AMENDED_PETITION</th>\n",
       "      <th>FULL_TIME_POSITION</th>\n",
       "      <th>PREVAILING_WAGE</th>\n",
       "      <th>PW_UNIT_OF_PAY</th>\n",
       "      <th>WAGE_RATE_OF_PAY_FROM</th>\n",
       "      <th>WAGE_RATE_OF_PAY_TO</th>\n",
       "      <th>WAGE_UNIT_OF_PAY</th>\n",
       "      <th>H1B_DEPENDENT</th>\n",
       "      <th>WILLFUL_VIOLATOR</th>\n",
       "      <th>WORKSITE_STATE</th>\n",
       "      <th>WORKSITE_POSTAL_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>I-200-18243-981567</td>\n",
       "      <td>CERTIFIED</td>\n",
       "      <td>2018-08-31</td>\n",
       "      <td>2018-09-07</td>\n",
       "      <td>H-1B</td>\n",
       "      <td>2018-09-24</td>\n",
       "      <td>2021-09-23</td>\n",
       "      <td>ADVANCED MICRO DEVICES, INC.</td>\n",
       "      <td>CA</td>\n",
       "      <td>95054</td>\n",
       "      <td>Y</td>\n",
       "      <td>HR BUSINESS PARTNER MANAGER</td>\n",
       "      <td>11-3121</td>\n",
       "      <td>HUMAN RESOURCES MANAGERS</td>\n",
       "      <td>334413.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>131972.000000</td>\n",
       "      <td>Year</td>\n",
       "      <td>131972.000000</td>\n",
       "      <td>178282.000000</td>\n",
       "      <td>Year</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>TX</td>\n",
       "      <td>78735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>I-200-18243-060543</td>\n",
       "      <td>CERTIFIED</td>\n",
       "      <td>2018-08-31</td>\n",
       "      <td>2018-09-07</td>\n",
       "      <td>H-1B</td>\n",
       "      <td>2018-08-31</td>\n",
       "      <td>2021-08-31</td>\n",
       "      <td>MASTECH DIGITAL TECHNOLOGIES, INC., A MASTECH ...</td>\n",
       "      <td>PA</td>\n",
       "      <td>15108</td>\n",
       "      <td>N</td>\n",
       "      <td>SENIOR SOFTWARE DEVELOPER</td>\n",
       "      <td>15-1132</td>\n",
       "      <td>SOFTWARE DEVELOPERS, APPLICATIONS</td>\n",
       "      <td>541511.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>50.619999</td>\n",
       "      <td>Hour</td>\n",
       "      <td>50.619999</td>\n",
       "      <td>50.619999</td>\n",
       "      <td>Hour</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>PA</td>\n",
       "      <td>19103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>I-200-18239-164083</td>\n",
       "      <td>CERTIFIED</td>\n",
       "      <td>2018-08-27</td>\n",
       "      <td>2018-08-31</td>\n",
       "      <td>H-1B</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2020-09-19</td>\n",
       "      <td>UNIVERSITY OF WASHINGTON</td>\n",
       "      <td>WA</td>\n",
       "      <td>98195</td>\n",
       "      <td>N</td>\n",
       "      <td>RESEARCH ASSOCIATE</td>\n",
       "      <td>19-1021</td>\n",
       "      <td>BIOCHEMISTS AND BIOPHYSICISTS</td>\n",
       "      <td>611310.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>41579.000000</td>\n",
       "      <td>Year</td>\n",
       "      <td>48432.000000</td>\n",
       "      <td>48432.000000</td>\n",
       "      <td>Year</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>WA</td>\n",
       "      <td>98195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            CASE_NUMBER CASE_STATUS CASE_SUBMITTED DECISION_DATE VISA_CLASS  \\\n",
       "688  I-200-18243-981567   CERTIFIED     2018-08-31    2018-09-07       H-1B   \n",
       "689  I-200-18243-060543   CERTIFIED     2018-08-31    2018-09-07       H-1B   \n",
       "690  I-200-18239-164083   CERTIFIED     2018-08-27    2018-08-31       H-1B   \n",
       "\n",
       "    EMPLOYMENT_START_DATE EMPLOYMENT_END_DATE  \\\n",
       "688            2018-09-24          2021-09-23   \n",
       "689            2018-08-31          2021-08-31   \n",
       "690            2018-09-20          2020-09-19   \n",
       "\n",
       "                                         EMPLOYER_NAME EMPLOYER_STATE  \\\n",
       "688                       ADVANCED MICRO DEVICES, INC.             CA   \n",
       "689  MASTECH DIGITAL TECHNOLOGIES, INC., A MASTECH ...             PA   \n",
       "690                           UNIVERSITY OF WASHINGTON             WA   \n",
       "\n",
       "    EMPLOYER_POSTAL_CODE AGENT_REPRESENTING_EMPLOYER  \\\n",
       "688                95054                           Y   \n",
       "689                15108                           N   \n",
       "690                98195                           N   \n",
       "\n",
       "                       JOB_TITLE SOC_CODE                           SOC_NAME  \\\n",
       "688  HR BUSINESS PARTNER MANAGER  11-3121           HUMAN RESOURCES MANAGERS   \n",
       "689    SENIOR SOFTWARE DEVELOPER  15-1132  SOFTWARE DEVELOPERS, APPLICATIONS   \n",
       "690           RESEARCH ASSOCIATE  19-1021      BIOCHEMISTS AND BIOPHYSICISTS   \n",
       "\n",
       "     NAICS_CODE  TOTAL_WORKERS  NEW_EMPLOYMENT  CONTINUED_EMPLOYMENT  \\\n",
       "688    334413.0              1               0                     0   \n",
       "689    541511.0              6               1                     1   \n",
       "690    611310.0              1               1                     0   \n",
       "\n",
       "     CHANGE_PREVIOUS_EMPLOYMENT  NEW_CONCURRENT_EMP  CHANGE_EMPLOYER  \\\n",
       "688                           0                   0                1   \n",
       "689                           1                   1                1   \n",
       "690                           0                   0                0   \n",
       "\n",
       "     AMENDED_PETITION FULL_TIME_POSITION  PREVAILING_WAGE PW_UNIT_OF_PAY  \\\n",
       "688                 0                  Y    131972.000000           Year   \n",
       "689                 1                  Y        50.619999           Hour   \n",
       "690                 0                  Y     41579.000000           Year   \n",
       "\n",
       "     WAGE_RATE_OF_PAY_FROM  WAGE_RATE_OF_PAY_TO WAGE_UNIT_OF_PAY  \\\n",
       "688          131972.000000        178282.000000             Year   \n",
       "689              50.619999            50.619999             Hour   \n",
       "690           48432.000000         48432.000000             Year   \n",
       "\n",
       "    H1B_DEPENDENT WILLFUL_VIOLATOR WORKSITE_STATE WORKSITE_POSTAL_CODE  \n",
       "688             N                N             TX                78735  \n",
       "689             Y                N             PA                19103  \n",
       "690             N                N             WA                98195  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Note the Differences between the two dataframes below!!!!! \n",
    "# # TestTestTest\n",
    "h1b_df_clean5.iloc[688:691]  # before resetting index\n",
    "h1b_df_clean6.iloc[688:691]  # after resetting index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D. Verifying the Imputation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells with a value of zero in `FROM` column: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PREVAILING_WAGE</th>\n",
       "      <th>WAGE_RATE_OF_PAY_TO</th>\n",
       "      <th>WAGE_RATE_OF_PAY_FROM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112549.0</td>\n",
       "      <td>143915.0</td>\n",
       "      <td>143915.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79976.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77792.0</td>\n",
       "      <td>78240.0</td>\n",
       "      <td>78240.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PREVAILING_WAGE  WAGE_RATE_OF_PAY_TO  WAGE_RATE_OF_PAY_FROM\n",
       "0         112549.0             143915.0               143915.0\n",
       "1          79976.0             100000.0               100000.0\n",
       "2          77792.0              78240.0                78240.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Verifying that `FROM` column doesn't contain any zero values\n",
    "count_zero = len(np.where(h1b_df_clean5.WAGE_RATE_OF_PAY_FROM == 0)[0])\n",
    "print(\"Number of cells with a value of zero in `FROM` column: {}\".format(count_zero))\n",
    "\n",
    "# # Verifying the imputation worked as expected\n",
    "h1b_df_clean5[['PREVAILING_WAGE','WAGE_RATE_OF_PAY_TO','WAGE_RATE_OF_PAY_FROM']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E. New Dataframe Dimensions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(654348, 32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Note the change in rows\n",
    "h1b_df_clean6.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#Table of Contents'>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Prevailing Wage'></a>\n",
    "____\n",
    "\n",
    "#### Note on `PREVAILING_WAGE`\n",
    "\n",
    "By the above logic in <a href='#Dataset Cleaning 2'>5a</a>, I could have regarded the zero values in PREVAILING_WAGE as intrinsically missing values and converted to non-zero integers as well. There is however a major reason why I didn't perform this manipulation and decided to drop these rows instead. \n",
    "\n",
    "1. It's really hard to accurately impute a value for `PREVAILING WAGE` due to the fact that every labor sub-area has a different prevailing wage. Therefore the typical descriptive statistics method to fill in missing values will not work. \n",
    "\n",
    "Therefore, we will be dropping the rows where the values are either zero or `nan` in `PREVAILING WAGE` column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Dropping zero values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check how many rows have zero values in PREVAILING WAGE column\n",
    "len(np.where(h1b_df_clean6.PREVAILING_WAGE == 0)[0])\n",
    "\n",
    "# # Check how many rows have nan values in PREVAILING WAGE column\n",
    "len(np.where(h1b_df_clean6.PREVAILING_WAGE.isnull())[0])\n",
    "\n",
    "# # Getting the index where value is 0\n",
    "wage_zero_idx = np.where(h1b_df_clean6.PREVAILING_WAGE == 0)[0]\n",
    "# h1b_df_clean5.loc[wage_zero_idx]  # investigating the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2458,   6193,   6775,   9724,   9739,   9839,  14599,  14731,\n",
       "        17007,  17011,  17508,  18725,  19368,  20568,  20873,  21035,\n",
       "        21321,  21530,  21611,  21803,  22184,  25268,  25304,  25633,\n",
       "        25725,  26968,  28906,  29018,  29487,  30074,  30723,  31312,\n",
       "        31551,  31607,  32244,  32785,  33506,  33984,  37300,  37653,\n",
       "        38322,  39653,  42355,  46111,  46563,  47516,  49081,  51927,\n",
       "        53364,  53866,  54012,  54132,  57147,  57698,  60523,  61749,\n",
       "       186618, 255833, 601816])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Example of 'zero_idx'\n",
    "wage_zero_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Be careful with dealing with row-wise missing values! Always remember to reset_index and don't use inplace operations! (It will push back the original index and propagate any indexed-based filteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(654289, 32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Drop the zero-value indices and reset the index! \n",
    "h1b_df_clean7 = h1b_df_clean6.drop(wage_zero_idx)\n",
    "h1b_df_clean8 = h1b_df_clean7.reset_index(drop=True)\n",
    "\n",
    "h1b_df_clean8.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Dropping null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(654286, 32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Getting the index where it is null (get_index should be done AFTER dropping/resetting_index)\n",
    "# # With the most recent cleaned dataframe\n",
    "wage_null_idx = np.where(h1b_df_clean8.PREVAILING_WAGE.isnull())[0]\n",
    "# h1b_df_clean5.loc[wage_null_idx]  # investigating the indices\n",
    "\n",
    "# # Drop the null indices and reset the index! \n",
    "h1b_df_clean9 = h1b_df_clean8.drop(wage_null_idx)\n",
    "h1b_df_clean10 = h1b_df_clean9.reset_index(drop=True)\n",
    "\n",
    "h1b_df_clean10.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**:  \n",
    "*For more information / investigation, play with the below dataframes*  \n",
    "Try chaning the suffix # to different values (6~10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example regardin zero-value index\n",
    "# h1b_df_clean9.loc[2457:2460, ['PREVAILING_WAGE','PW_UNIT_OF_PAY']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Verification Step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells with a value of zero in `Prevaling Wage` column: 0\n",
      "Number of cells with nan values in `Prevaling Wage` column: 0\n"
     ]
    }
   ],
   "source": [
    "# # Verifying that `PREVAILING WAGE` column doesn't contain any zero values or null values\n",
    "pw_zero = len(np.where(h1b_df_clean10.PREVAILING_WAGE == 0)[0])\n",
    "print(\"Number of cells with a value of zero in `Prevaling Wage` column: {}\".format(pw_zero))\n",
    "\n",
    "pw_null = len(np.where(h1b_df_clean10.PREVAILING_WAGE.isnull())[0])\n",
    "print(\"Number of cells with nan values in `Prevaling Wage` column: {}\".format(pw_null))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#Table of Contents'>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Dataset Cleaning 2b'></a>\n",
    "____\n",
    "\n",
    "### 5b. Investigating Missing Values\n",
    "\n",
    "Now let's actually take a look at any missing values in the rows. We utilize the `find_cols_to_drop` funtion to investigate any remaining missing values. Using print statements, we can identify which columns still have missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Modified `find_cols_to_drop` Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING ROWS per COLUMN\n",
      "CASE_NUMBER: 0, 0.00%\n",
      "CASE_STATUS: 0, 0.00%\n",
      "CASE_SUBMITTED: 1, 0.00%\n",
      "DECISION_DATE: 0, 0.00%\n",
      "VISA_CLASS: 0, 0.00%\n",
      "EMPLOYMENT_START_DATE: 3, 0.00%\n",
      "EMPLOYMENT_END_DATE: 6, 0.00%\n",
      "EMPLOYER_NAME: 13, 0.00%\n",
      "EMPLOYER_STATE: 67, 0.01%\n",
      "EMPLOYER_POSTAL_CODE: 12, 0.00%\n",
      "AGENT_REPRESENTING_EMPLOYER: 10, 0.00%\n",
      "JOB_TITLE: 0, 0.00%\n",
      "SOC_CODE: 3, 0.00%\n",
      "SOC_NAME: 3, 0.00%\n",
      "NAICS_CODE: 1, 0.00%\n",
      "TOTAL_WORKERS: 0, 0.00%\n",
      "NEW_EMPLOYMENT: 0, 0.00%\n",
      "CONTINUED_EMPLOYMENT: 0, 0.00%\n",
      "CHANGE_PREVIOUS_EMPLOYMENT: 0, 0.00%\n",
      "NEW_CONCURRENT_EMP: 0, 0.00%\n",
      "CHANGE_EMPLOYER: 0, 0.00%\n",
      "AMENDED_PETITION: 0, 0.00%\n",
      "FULL_TIME_POSITION: 2, 0.00%\n",
      "PREVAILING_WAGE: 0, 0.00%\n",
      "PW_UNIT_OF_PAY: 1, 0.00%\n",
      "WAGE_RATE_OF_PAY_FROM: 0, 0.00%\n",
      "WAGE_RATE_OF_PAY_TO: 0, 0.00%\n",
      "WAGE_UNIT_OF_PAY: 3, 0.00%\n",
      "H1B_DEPENDENT: 14132, 2.16%\n",
      "WILLFUL_VIOLATOR: 14137, 2.16%\n",
      "WORKSITE_STATE: 4, 0.00%\n",
      "WORKSITE_POSTAL_CODE: 17, 0.00%\n"
     ]
    }
   ],
   "source": [
    "# # Adapted from https://github.com/dametreusv/world_development_indicators_data_science/blob/master/WDI_wrangle.ipynb\n",
    "\n",
    "def display_missing_rows(df):\n",
    "    columns = df.columns\n",
    "    print('MISSING ROWS per COLUMN')\n",
    "    for column in columns:\n",
    "        percentage = (df[column].isnull().sum() / df.shape[0]) * 100\n",
    "        print('{}: {}, {:0.2f}%'.format(column, df[column].isnull().sum(), percentage))\n",
    "\n",
    "# # Call the function\n",
    "# h1b_df_clean4 is the final cleaned df from above \n",
    "# (even after value imputation for WAGE column)\n",
    "display_missing_rows(h1b_df_clean10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** We can see that the most of the columns now have close to 0% missing values. Two columns however stand out - `H1B_DEPENDENT` & `WILLFUL_VIOLATOR`. Interestingly they have nearly identical number of missing values. Let's investigate this phenomenon further to see if we can observe any patterns / gain insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Investigating missing values in `H1B_DEPENDENT` & `WILLFUL_VIOLATOR` columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 15 null value index of H1B_DEPENDENT column:\n",
      "[  8  41  73  96 197 293 417 480 578 588 589 613 631 703 717]\n",
      "First 15 null value index of WILLFUL_VIOLATOR column:\n",
      "[  8  41  73  96 197 293 417 480 578 588 589 613 631 703 717]\n"
     ]
    }
   ],
   "source": [
    "# Print out the first 15 indexes where the columns have nan values\n",
    "\n",
    "hd_null_idx = np.where(h1b_df_clean10.H1B_DEPENDENT.isnull())[0]\n",
    "print(\"First 15 null value index of H1B_DEPENDENT column:\\n{}\".format(hd_null_idx[:15]))\n",
    "wv_null_idx= np.where(h1b_df_clean10.WILLFUL_VIOLATOR.isnull())[0]\n",
    "print(\"First 15 null value index of WILLFUL_VIOLATOR column:\\n{}\".format(wv_null_idx[:15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** This is very interesting! The first 20 indices at which the columns have missing (nan) values are identical. I wonder if there are more shared indices between the missing values of the two columns. To investigate that, we use the `set()` function to locate the intersecting indices between two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14131 shared indices.\n"
     ]
    }
   ],
   "source": [
    "shared_idx = list(set(hd_null_idx) & set(wv_null_idx))\n",
    "len(shared_idx)\n",
    "print(\"There are {} shared indices.\".format(len(shared_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** WOW! There are 14131 shared indices that are shared between the missing values of the two columns. This is more than a coincidence. Let's index the cleaned dataframe with the above shared index to see if we can gain some insight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASE_NUMBER</th>\n",
       "      <th>CASE_STATUS</th>\n",
       "      <th>CASE_SUBMITTED</th>\n",
       "      <th>DECISION_DATE</th>\n",
       "      <th>VISA_CLASS</th>\n",
       "      <th>EMPLOYMENT_START_DATE</th>\n",
       "      <th>EMPLOYMENT_END_DATE</th>\n",
       "      <th>EMPLOYER_NAME</th>\n",
       "      <th>EMPLOYER_STATE</th>\n",
       "      <th>EMPLOYER_POSTAL_CODE</th>\n",
       "      <th>AGENT_REPRESENTING_EMPLOYER</th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>SOC_CODE</th>\n",
       "      <th>SOC_NAME</th>\n",
       "      <th>NAICS_CODE</th>\n",
       "      <th>TOTAL_WORKERS</th>\n",
       "      <th>NEW_EMPLOYMENT</th>\n",
       "      <th>CONTINUED_EMPLOYMENT</th>\n",
       "      <th>CHANGE_PREVIOUS_EMPLOYMENT</th>\n",
       "      <th>NEW_CONCURRENT_EMP</th>\n",
       "      <th>CHANGE_EMPLOYER</th>\n",
       "      <th>AMENDED_PETITION</th>\n",
       "      <th>FULL_TIME_POSITION</th>\n",
       "      <th>PREVAILING_WAGE</th>\n",
       "      <th>PW_UNIT_OF_PAY</th>\n",
       "      <th>WAGE_RATE_OF_PAY_FROM</th>\n",
       "      <th>WAGE_RATE_OF_PAY_TO</th>\n",
       "      <th>WAGE_UNIT_OF_PAY</th>\n",
       "      <th>H1B_DEPENDENT</th>\n",
       "      <th>WILLFUL_VIOLATOR</th>\n",
       "      <th>WORKSITE_STATE</th>\n",
       "      <th>WORKSITE_POSTAL_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32770</th>\n",
       "      <td>I-203-18211-042853</td>\n",
       "      <td>DENIED</td>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>2018-08-01</td>\n",
       "      <td>E-3 Australian</td>\n",
       "      <td>2018-08-13</td>\n",
       "      <td>2018-10-10</td>\n",
       "      <td>THE OFFICE PERFORMING ARTS + FILM, INC.</td>\n",
       "      <td>NY</td>\n",
       "      <td>10014</td>\n",
       "      <td>N</td>\n",
       "      <td>COMPANY MANAGER</td>\n",
       "      <td>27-1019</td>\n",
       "      <td>ARTISTS AND RELATED WORKERS, ALL OTHER</td>\n",
       "      <td>71119.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>37.02</td>\n",
       "      <td>Hour</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>Week</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NY</td>\n",
       "      <td>10014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294915</th>\n",
       "      <td>I-203-18003-406124</td>\n",
       "      <td>CERTIFIED</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>E-3 Australian</td>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>2020-01-15</td>\n",
       "      <td>RECRUITERLY, INC.</td>\n",
       "      <td>CA</td>\n",
       "      <td>94607</td>\n",
       "      <td>Y</td>\n",
       "      <td>CHIEF FINANCIAL OFFICER</td>\n",
       "      <td>11-3031</td>\n",
       "      <td>FINANCIAL MANAGERS</td>\n",
       "      <td>561320.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>89232.00</td>\n",
       "      <td>Year</td>\n",
       "      <td>89232.0</td>\n",
       "      <td>89232.0</td>\n",
       "      <td>Year</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CA</td>\n",
       "      <td>94607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65539</th>\n",
       "      <td>I-203-17310-627935</td>\n",
       "      <td>CERTIFIED</td>\n",
       "      <td>2017-11-07</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>E-3 Australian</td>\n",
       "      <td>2017-11-07</td>\n",
       "      <td>2019-11-06</td>\n",
       "      <td>CNSDOSE, LLC</td>\n",
       "      <td>TX</td>\n",
       "      <td>70095</td>\n",
       "      <td>Y</td>\n",
       "      <td>CHIEF MEDICAL OFFICER</td>\n",
       "      <td>11-9111</td>\n",
       "      <td>MEDICAL AND HEALTH SERVICES MANAGERS</td>\n",
       "      <td>511210.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>68370.00</td>\n",
       "      <td>Year</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>Year</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TX</td>\n",
       "      <td>70095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               CASE_NUMBER CASE_STATUS CASE_SUBMITTED DECISION_DATE  \\\n",
       "32770   I-203-18211-042853      DENIED     2018-07-30    2018-08-01   \n",
       "294915  I-203-18003-406124   CERTIFIED     2018-01-03    2018-01-09   \n",
       "65539   I-203-17310-627935   CERTIFIED     2017-11-07    2017-11-14   \n",
       "\n",
       "            VISA_CLASS EMPLOYMENT_START_DATE EMPLOYMENT_END_DATE  \\\n",
       "32770   E-3 Australian            2018-08-13          2018-10-10   \n",
       "294915  E-3 Australian            2018-01-15          2020-01-15   \n",
       "65539   E-3 Australian            2017-11-07          2019-11-06   \n",
       "\n",
       "                                  EMPLOYER_NAME EMPLOYER_STATE  \\\n",
       "32770   THE OFFICE PERFORMING ARTS + FILM, INC.             NY   \n",
       "294915                        RECRUITERLY, INC.             CA   \n",
       "65539                              CNSDOSE, LLC             TX   \n",
       "\n",
       "       EMPLOYER_POSTAL_CODE AGENT_REPRESENTING_EMPLOYER  \\\n",
       "32770                 10014                           N   \n",
       "294915                94607                           Y   \n",
       "65539                 70095                           Y   \n",
       "\n",
       "                      JOB_TITLE SOC_CODE  \\\n",
       "32770           COMPANY MANAGER  27-1019   \n",
       "294915  CHIEF FINANCIAL OFFICER  11-3031   \n",
       "65539     CHIEF MEDICAL OFFICER  11-9111   \n",
       "\n",
       "                                      SOC_NAME  NAICS_CODE  TOTAL_WORKERS  \\\n",
       "32770   ARTISTS AND RELATED WORKERS, ALL OTHER     71119.0              1   \n",
       "294915                      FINANCIAL MANAGERS    561320.0              1   \n",
       "65539     MEDICAL AND HEALTH SERVICES MANAGERS    511210.0              1   \n",
       "\n",
       "        NEW_EMPLOYMENT  CONTINUED_EMPLOYMENT  CHANGE_PREVIOUS_EMPLOYMENT  \\\n",
       "32770                1                     0                           0   \n",
       "294915               1                     0                           0   \n",
       "65539                1                     0                           0   \n",
       "\n",
       "        NEW_CONCURRENT_EMP  CHANGE_EMPLOYER  AMENDED_PETITION  \\\n",
       "32770                    0                0                 0   \n",
       "294915                   0                0                 0   \n",
       "65539                    0                0                 0   \n",
       "\n",
       "       FULL_TIME_POSITION  PREVAILING_WAGE PW_UNIT_OF_PAY  \\\n",
       "32770                   Y            37.02           Hour   \n",
       "294915                  Y         89232.00           Year   \n",
       "65539                   Y         68370.00           Year   \n",
       "\n",
       "        WAGE_RATE_OF_PAY_FROM  WAGE_RATE_OF_PAY_TO WAGE_UNIT_OF_PAY  \\\n",
       "32770                  1650.0               1650.0             Week   \n",
       "294915                89232.0              89232.0             Year   \n",
       "65539                 80000.0              80000.0             Year   \n",
       "\n",
       "       H1B_DEPENDENT WILLFUL_VIOLATOR WORKSITE_STATE WORKSITE_POSTAL_CODE  \n",
       "32770            NaN              NaN             NY                10014  \n",
       "294915           NaN              NaN             CA                94607  \n",
       "65539            NaN              NaN             TX                70095  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Accessing the cleaned dataframe (h1b_df_clean10) with the shared index from above\n",
    "# # print out only the first n rows\n",
    "h1b_df_clean10.iloc[shared_idx].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** One thing stands out. The VISA class in this particular dataframe is either `E-3 Australian` or `H-1B1 Chile`. Let's investigate further by looking at value_counts() of the `VISA_CLASS` column of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "E-3 Australian     12003\n",
       "H-1B1 Singapore     1191\n",
       "H-1B1 Chile          930\n",
       "H-1B                   7\n",
       "Name: VISA_CLASS, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Looking at value_counts() of VISA_CLASS column\n",
    "nan_df = h1b_df_clean10.iloc[shared_idx]\n",
    "nan_df_vc = nan_df.VISA_CLASS.value_counts()\n",
    "nan_df_vc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** It looks like `H1B_DEPENDENT` & `WILLFUL_VIOLATOR` columns had missing values because the `VISA_CLASS` was different. However we should compare the above values with the total number of VISA CLASS observed from the entire dataset to be sure that the VISA CLASS is a factor in determining the values of `H1B_DEPENDENT` & `WILLFUL_VIOLATOR` columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of VISA_CLASS counts compared to original dataset:\n",
      "H1-B: 0.00%\n",
      "Australia: 95.67%\n",
      "Chile: 92.35%\n",
      "Singapore: 94.08%\n"
     ]
    }
   ],
   "source": [
    "# # Using h1b_df_clean4_vc as the reference dataframe\n",
    "h1b_df_clean10_vc = h1b_df_clean10.VISA_CLASS.value_counts()\n",
    "\n",
    "us = nan_df_vc['H-1B']/h1b_df_clean10_vc['H-1B'] * 100\n",
    "aus = nan_df_vc['E-3 Australian']/h1b_df_clean10_vc['E-3 Australian'] * 100\n",
    "chile = nan_df_vc['H-1B1 Chile']/h1b_df_clean10_vc['H-1B1 Chile'] * 100\n",
    "sing = nan_df_vc['H-1B1 Singapore']/h1b_df_clean10_vc['H-1B1 Singapore'] * 100\n",
    "\n",
    "print(\"Percentage of VISA_CLASS counts compared to original dataset:\\nH1-B: {:.2f}%\\nAustralia: {:.2f}%\\nChile: {:.2f}%\\nSingapore: {:.2f}%\".format(us, aus, chile, sing))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** So it looks like not all rows with **Australia, Chile, or Singapore** VISA CLASS had `H1B_DEPENDENT` & `WILLFUL_VIOLATOR` columns missing. Since we don't know whether this is a significant feature or not, let's impute the missing column values with ***N/A*** *(Not Applicable)* to retain as much information as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Filling in missing values for `H1B_DEPENDENT` & `WILLFUL_VIOLATOR` columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replace the missing values with the string N/A\n",
    "# # Must \"add\" the category first (see Note Below)\n",
    "\n",
    "h1b_df_clean10['H1B_DEPENDENT'] = h1b_df_clean10['H1B_DEPENDENT'].cat.add_categories(\"N/A\").fillna(\"N/A\")\n",
    "h1b_df_clean10['WILLFUL_VIOLATOR'] = h1b_df_clean10['WILLFUL_VIOLATOR'].cat.add_categories(\"N/A\").fillna(\"N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** Because the columns are of `Category` dtype, must [\"add\"](https://stackoverflow.com/questions/32718639/pandas-filling-nans-in-categorical-data/36193135) the relevant category first before filling in the missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E. Check Missing Values per Column after Value Imputation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(654286, 32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Commented Out to save Space\n",
    "# display_missing_rows(h1b_df_clean10)\n",
    "# # Dimensions of cleaned dataframe\n",
    "h1b_df_clean10.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#Table of Contents'>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Dataset Cleaning 2c'></a>\n",
    "____\n",
    "\n",
    "### 5c. Imputing missing values based on existing column relationship - STATE & POSTAL CODE\n",
    "\n",
    "Most of the columns are now free of missing values and most of the missing values are now concentrated in `STATE` or `POSTAL_CODE` columns. Thankfully however, there is a *one* ***(state)***-to-*many* ***(postal_codes)*** relationship between states and postal codes. This relationship allows us to impute values for `STATE` column from the `POSTAL_CODE` column but not the other way around. As long as another row exists that contains the relationship between `STATE` column and `POSTAL_CODE` column, we will be able to successfully impute missing values for `STATE` column.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be doing the following steps to sucessfully impute `STATE` values based on `POSTAL_CODE` values. \n",
    "\n",
    "1. Make a dictionary from `STATE` and `POSTAL_CODE` data from the cleaned dataset (`h1b_df_clean10`)\n",
    "   - Due to the **many-to-one relationship**, the keys of the dictionary must be 'many' (POSTAL_CODE) \n",
    "   - Make sure that the key:value pair of the dictionary is accurate (more on this <a href='#mode of values'>later</a>)\n",
    "   \n",
    "  \n",
    "2. Using the dictionary, I will create a new column of STATES mapped from the dictionary.\n",
    "3. Drop the original STATE column and rename the new STATE column accordingly. \n",
    "   \n",
    "    \n",
    "**(Summary):**\n",
    "From *Postal_Code* data $\\rightarrow$ fill in *State* data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Investigating the missing values for `STATE` column:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 67 rows with missing STATE values\n"
     ]
    }
   ],
   "source": [
    "state_null = np.where(h1b_df_clean10.EMPLOYER_STATE.isnull())[0]\n",
    "state_null_df = h1b_df_clean10.loc[state_null]\n",
    "state_null_df.shape\n",
    "\n",
    "print(\"There are {} rows with missing STATE values\".format(state_null_df.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mode of values'></a>\n",
    "**B. Make a dictionary from `STATE` and `POSTAL CODE` columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before blindly creating a dictionary from these two columns, we need to: \n",
    "1. sort the keys \n",
    "2. verify that the key:value pair is valid \n",
    "    - **Note**: Due to human error, zipcode and state could have been inputted incorrectly at the time of dataset creation. If for some reason after sorting by **keys**, the incorrect **value** is lexically behind the correct **value**, then the dictionary will have an incorrect key:value pair and the error will propagate. To prevent these edge cases from affecting our data integrity, we rely on the wisdom of the masses. Basically, we treat the *mode* of the **value** as a valid **value**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sort the Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Postal Codes: 10771\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMPLOYER_STATE</th>\n",
       "      <th>EMPLOYER_POSTAL_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>583613</th>\n",
       "      <td>WV</td>\n",
       "      <td>98052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638401</th>\n",
       "      <td>WA</td>\n",
       "      <td>98052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213485</th>\n",
       "      <td>WA</td>\n",
       "      <td>98052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       EMPLOYER_STATE EMPLOYER_POSTAL_CODE\n",
       "583613             WV                98052\n",
       "638401             WA                98052\n",
       "213485             WA                98052"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Create a 'state df' that only contains two columns of our interest\n",
    "# # Drop any nan values to prevent nans getting into the dictionary\n",
    "state_df = h1b_df_clean10[['EMPLOYER_STATE', 'EMPLOYER_POSTAL_CODE']].dropna(how='any')\n",
    "\n",
    "# # sort by postal code (our keys)\n",
    "state_df = state_df.sort_values(by='EMPLOYER_POSTAL_CODE')\n",
    "\n",
    "# # # of Unique Postal Codes \n",
    "u_num = state_df.EMPLOYER_POSTAL_CODE.unique().shape[0]\n",
    "print(\"Number of Unique Postal Codes: {}\".format(u_num))\n",
    "# # Example Case of how wrong key:value pair can be introduced into the dictionary\n",
    "state_df.loc[state_df.EMPLOYER_POSTAL_CODE == '98052'].sort_values(by='EMPLOYER_STATE',ascending=False).head(3)\n",
    "\n",
    "# # Here, we can see that the correct State corresponding to a zipcode of 98052 is 'WA'. \n",
    "# # However, if we sort incorrectly, the corresponding value pair for the key 98052 will be incorrectly set to 'WV'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Function that will be applied to the Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mode(inp):\n",
    "    inp_df = state_df[state_df.EMPLOYER_POSTAL_CODE == inp]          # Filters dataframe by the Postal Code\n",
    "    output = inp_df.EMPLOYER_STATE.value_counts().index.tolist()[0]  # Value counts returns the sorted index\n",
    "    \n",
    "    return output \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION: ASK BEN ABOUT PERFORMANCE (of .apply())\n",
    "\n",
    "- apply method is very slow: any way to increase the performance??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make a new dataframe and apply the function to the column (EMPLOYER_STATE)\n",
    "state_df2 = state_df.copy()\n",
    "\n",
    "# # Map STATE column based on POSTAL CODE values \n",
    "state_df2.EMPLOYER_STATE = state_df2.EMPLOYER_POSTAL_CODE.apply(apply_mode)  \n",
    "# # outputs mode to employer state column\n",
    "\n",
    "# # PERFORMANCE IS VERY SLOW!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Verify the validity of key:value pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WA    6453\n",
       "Name: EMPLOYER_STATE, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Verify that the apply function worked\n",
    "# # This particular example (state_df2) should only have 'WA' as output\n",
    "\n",
    "# state_df.loc[state_df.EMPLOYER_POSTAL_CODE == '98052'].EMPLOYER_STATE.value_counts()\n",
    "\n",
    "state_df2.loc[state_df2.EMPLOYER_POSTAL_CODE == '98052'].EMPLOYER_STATE.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Make the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Making a dictionary with state_df2\n",
    "# # Make sure that POSTAL_CODE is keys()!\n",
    "\n",
    "state_dict = dict(zip(state_df2.EMPLOYER_POSTAL_CODE, state_df2.EMPLOYER_STATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Create a New Dataframe with the new column mapped from the dictionary + Rearrange Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a copy and make a new column + Rearrange Columns\n",
    "h1b_df_clean11 = h1b_df_clean10.copy()\n",
    "h1b_df_clean11['EMPLOYER_STATE_MAPPED'] = h1b_df_clean11['EMPLOYER_POSTAL_CODE'].map(state_dict)\n",
    "\n",
    "h1b_df_clean11 = h1b_df_clean11[['CASE_NUMBER', 'CASE_STATUS', 'CASE_SUBMITTED', 'DECISION_DATE',\n",
    "       'VISA_CLASS', 'EMPLOYMENT_START_DATE', 'EMPLOYMENT_END_DATE',\n",
    "       'EMPLOYER_NAME', 'EMPLOYER_STATE_MAPPED', 'EMPLOYER_STATE','EMPLOYER_POSTAL_CODE', 'AGENT_REPRESENTING_EMPLOYER',\n",
    "       'JOB_TITLE', 'SOC_CODE', 'SOC_NAME', 'NAICS_CODE', 'TOTAL_WORKERS',\n",
    "       'NEW_EMPLOYMENT', 'CONTINUED_EMPLOYMENT', 'CHANGE_PREVIOUS_EMPLOYMENT',\n",
    "       'NEW_CONCURRENT_EMP', 'CHANGE_EMPLOYER', 'AMENDED_PETITION',\n",
    "       'FULL_TIME_POSITION', 'PREVAILING_WAGE', 'PW_UNIT_OF_PAY',\n",
    "       'WAGE_RATE_OF_PAY_FROM', 'WAGE_RATE_OF_PAY_TO', 'WAGE_UNIT_OF_PAY',\n",
    "       'H1B_DEPENDENT', 'WILLFUL_VIOLATOR', 'WORKSITE_STATE',\n",
    "       'WORKSITE_POSTAL_CODE']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D. Check the Validity of Imputations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_series = h1b_df_clean11[['EMPLOYER_STATE_MAPPED']].EMPLOYER_STATE_MAPPED \n",
    "orig_series = h1b_df_clean11[['EMPLOYER_STATE']].EMPLOYER_STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463 values have been modified after mapping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMPLOYER_STATE_MAPPED</th>\n",
       "      <th>EMPLOYER_STATE</th>\n",
       "      <th>EMPLOYER_POSTAL_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>NE</td>\n",
       "      <td>MO</td>\n",
       "      <td>68114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>NE</td>\n",
       "      <td>MO</td>\n",
       "      <td>68114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>TX</td>\n",
       "      <td>MA</td>\n",
       "      <td>75024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3281</th>\n",
       "      <td>TX</td>\n",
       "      <td>OH</td>\n",
       "      <td>77042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3936</th>\n",
       "      <td>NY</td>\n",
       "      <td>NJ</td>\n",
       "      <td>11106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EMPLOYER_STATE_MAPPED EMPLOYER_STATE EMPLOYER_POSTAL_CODE\n",
       "587                     NE             MO                68114\n",
       "731                     NE             MO                68114\n",
       "2385                    TX             MA                75024\n",
       "3281                    TX             OH                77042\n",
       "3936                    NY             NJ                11106"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Find the number of / index at where the Mapped and the Original Values differ\n",
    "\n",
    "num_modified = len(np.where(mapped_series != orig_series)[0]) \n",
    "print(\"{} values have been modified after mapping\".format(num_modified))\n",
    "\n",
    "mod_idx = np.where(mapped_series != orig_series)[0]\n",
    "h1b_df_clean11.loc[mod_idx,['EMPLOYER_STATE_MAPPED','EMPLOYER_STATE','EMPLOYER_POSTAL_CODE']].head(5)\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULT**: 463 values differed between the mapped and the original columns. This means that potentially 463 values have been inputted incorrectly. We can easily check the validity of the imputation with a quick google search. For example at index 587, the state for Postal Code '68114' is in fact Omaha, NE (Nebraska), not MO (Missouri). Using the mode of the frequency, we were able to catch incorrect inputs and modify them accordingly, thereby increasing our data integrity! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E. Drop the original `EMPLOYER STATE` column and check dimensions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(654286, 32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Drop Original and Rearrange Columns\n",
    "h1b_df_clean12 = h1b_df_clean11.drop('EMPLOYER_STATE', axis=1)\n",
    "h1b_df_clean12.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#Table of Contents'>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Dataset Cleaning 2d'></a>\n",
    "____\n",
    "\n",
    "### 5d. POSTAL CODES that are *\"weird\"*\n",
    "\n",
    "Let's check our missing values again using the `display_missing_rows` function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING ROWS per COLUMN\n",
      "CASE_NUMBER: 0, 0.00%\n",
      "CASE_STATUS: 0, 0.00%\n",
      "CASE_SUBMITTED: 1, 0.00%\n",
      "DECISION_DATE: 0, 0.00%\n",
      "VISA_CLASS: 0, 0.00%\n",
      "EMPLOYMENT_START_DATE: 3, 0.00%\n",
      "EMPLOYMENT_END_DATE: 6, 0.00%\n",
      "EMPLOYER_NAME: 13, 0.00%\n",
      "EMPLOYER_STATE_MAPPED: 71, 0.01%\n",
      "EMPLOYER_POSTAL_CODE: 12, 0.00%\n",
      "AGENT_REPRESENTING_EMPLOYER: 10, 0.00%\n",
      "JOB_TITLE: 0, 0.00%\n",
      "SOC_CODE: 3, 0.00%\n",
      "SOC_NAME: 3, 0.00%\n",
      "NAICS_CODE: 1, 0.00%\n",
      "TOTAL_WORKERS: 0, 0.00%\n",
      "NEW_EMPLOYMENT: 0, 0.00%\n",
      "CONTINUED_EMPLOYMENT: 0, 0.00%\n",
      "CHANGE_PREVIOUS_EMPLOYMENT: 0, 0.00%\n",
      "NEW_CONCURRENT_EMP: 0, 0.00%\n",
      "CHANGE_EMPLOYER: 0, 0.00%\n",
      "AMENDED_PETITION: 0, 0.00%\n",
      "FULL_TIME_POSITION: 2, 0.00%\n",
      "PREVAILING_WAGE: 0, 0.00%\n",
      "PW_UNIT_OF_PAY: 1, 0.00%\n",
      "WAGE_RATE_OF_PAY_FROM: 0, 0.00%\n",
      "WAGE_RATE_OF_PAY_TO: 0, 0.00%\n",
      "WAGE_UNIT_OF_PAY: 3, 0.00%\n",
      "H1B_DEPENDENT: 0, 0.00%\n",
      "WILLFUL_VIOLATOR: 0, 0.00%\n",
      "WORKSITE_STATE: 4, 0.00%\n",
      "WORKSITE_POSTAL_CODE: 17, 0.00%\n"
     ]
    }
   ],
   "source": [
    "display_missing_rows(h1b_df_clean12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULT:** Interstingly, the missing values in newly mapped `EMPLOYER_STATE_MAPPED` column increased to 71 values from 67! This is probably due to the fact that `EMPLOYER_POSTAL_CODE` has 12 missing values as well. Let's drop the rows at which `EMPLOYER_POSTAL_CODE` values are null and re-investigate the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Drop missing values from `EMPLOYER_POSTAL_CODE` column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(654274, 32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Drop the Rows where EMPLOYER_POSTAL_CODE is null\n",
    "\n",
    "# # Getting the index where the column is null \n",
    "zipcode_null_idx = np.where(h1b_df_clean12.EMPLOYER_POSTAL_CODE.isnull())[0]\n",
    "\n",
    "# # Drop the null indices and reset the index! \n",
    "h1b_df_clean13 = h1b_df_clean12.drop(zipcode_null_idx)\n",
    "h1b_df_clean14 = h1b_df_clean13.reset_index(drop=True)\n",
    "\n",
    "h1b_df_clean14.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING ROWS per COLUMN\n",
      "CASE_NUMBER: 0, 0.00%\n",
      "CASE_STATUS: 0, 0.00%\n",
      "CASE_SUBMITTED: 1, 0.00%\n",
      "DECISION_DATE: 0, 0.00%\n",
      "VISA_CLASS: 0, 0.00%\n",
      "EMPLOYMENT_START_DATE: 3, 0.00%\n",
      "EMPLOYMENT_END_DATE: 6, 0.00%\n",
      "EMPLOYER_NAME: 12, 0.00%\n",
      "EMPLOYER_STATE_MAPPED: 59, 0.01%\n",
      "EMPLOYER_POSTAL_CODE: 0, 0.00%\n",
      "AGENT_REPRESENTING_EMPLOYER: 10, 0.00%\n",
      "JOB_TITLE: 0, 0.00%\n",
      "SOC_CODE: 3, 0.00%\n",
      "SOC_NAME: 3, 0.00%\n",
      "NAICS_CODE: 0, 0.00%\n",
      "TOTAL_WORKERS: 0, 0.00%\n",
      "NEW_EMPLOYMENT: 0, 0.00%\n",
      "CONTINUED_EMPLOYMENT: 0, 0.00%\n",
      "CHANGE_PREVIOUS_EMPLOYMENT: 0, 0.00%\n",
      "NEW_CONCURRENT_EMP: 0, 0.00%\n",
      "CHANGE_EMPLOYER: 0, 0.00%\n",
      "AMENDED_PETITION: 0, 0.00%\n",
      "FULL_TIME_POSITION: 2, 0.00%\n",
      "PREVAILING_WAGE: 0, 0.00%\n",
      "PW_UNIT_OF_PAY: 1, 0.00%\n",
      "WAGE_RATE_OF_PAY_FROM: 0, 0.00%\n",
      "WAGE_RATE_OF_PAY_TO: 0, 0.00%\n",
      "WAGE_UNIT_OF_PAY: 3, 0.00%\n",
      "H1B_DEPENDENT: 0, 0.00%\n",
      "WILLFUL_VIOLATOR: 0, 0.00%\n",
      "WORKSITE_STATE: 4, 0.00%\n",
      "WORKSITE_POSTAL_CODE: 17, 0.00%\n"
     ]
    }
   ],
   "source": [
    "display_missing_rows(h1b_df_clean14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULT:** After dropping the missing value rows from `EMPLOYER_POSTAL_CODE`, there are still  59 missing values left in the `EMPLOYER_STATE_MAPPED` column. This seems a bit high, so let's index into the dataframe to see if anything pops out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Investigating the missing values in `EMPLOYER_STATE_MAPPED` column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMPLOYER_NAME</th>\n",
       "      <th>EMPLOYER_STATE_MAPPED</th>\n",
       "      <th>EMPLOYER_POSTAL_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9290</th>\n",
       "      <td>GOLDEN OPPORTUNITY VENTURES INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13167</th>\n",
       "      <td>GOLDEN OPPORTUNITY VENTURES INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16092</th>\n",
       "      <td>COMPUTERTALK TECHNOLOGY INC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L3T7M8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17175</th>\n",
       "      <td>GOLDEN OPPORTUNITY VENTURES INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41583</th>\n",
       "      <td>GOPC PTY LTD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44535</th>\n",
       "      <td>GOLDEN OPPORTUNITY VENTURES INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48487</th>\n",
       "      <td>GOLDEN OPPORTUNITY VENTURES INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52499</th>\n",
       "      <td>GOLDEN OPPORTUNITY VENTURES INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56419</th>\n",
       "      <td>GOLDEN OPPORTUNITY VENTURES INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60235</th>\n",
       "      <td>GOLDEN OPPORTUNITY VENTURES INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70156</th>\n",
       "      <td>LULULEMON USA INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V6J 1C7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81946</th>\n",
       "      <td>PRACTIFI INC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91356</th>\n",
       "      <td>LULULEMON USA INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V6J 1C7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93086</th>\n",
       "      <td>LULULEMON USA INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V6J 1C7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104514</th>\n",
       "      <td>1-800-GOT-JUNK? LLC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V5T4T5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           EMPLOYER_NAME EMPLOYER_STATE_MAPPED  \\\n",
       "9290    GOLDEN OPPORTUNITY VENTURES INC.                   NaN   \n",
       "13167   GOLDEN OPPORTUNITY VENTURES INC.                   NaN   \n",
       "16092        COMPUTERTALK TECHNOLOGY INC                   NaN   \n",
       "17175   GOLDEN OPPORTUNITY VENTURES INC.                   NaN   \n",
       "41583                       GOPC PTY LTD                   NaN   \n",
       "44535   GOLDEN OPPORTUNITY VENTURES INC.                   NaN   \n",
       "48487   GOLDEN OPPORTUNITY VENTURES INC.                   NaN   \n",
       "52499   GOLDEN OPPORTUNITY VENTURES INC.                   NaN   \n",
       "56419   GOLDEN OPPORTUNITY VENTURES INC.                   NaN   \n",
       "60235   GOLDEN OPPORTUNITY VENTURES INC.                   NaN   \n",
       "70156                 LULULEMON USA INC.                   NaN   \n",
       "81946                       PRACTIFI INC                   NaN   \n",
       "91356                 LULULEMON USA INC.                   NaN   \n",
       "93086                 LULULEMON USA INC.                   NaN   \n",
       "104514               1-800-GOT-JUNK? LLC                   NaN   \n",
       "\n",
       "       EMPLOYER_POSTAL_CODE  \n",
       "9290                   6008  \n",
       "13167                  6008  \n",
       "16092                L3T7M8  \n",
       "17175                  6008  \n",
       "41583                  6850  \n",
       "44535                  6008  \n",
       "48487                  6008  \n",
       "52499                  6008  \n",
       "56419                  6008  \n",
       "60235                  6008  \n",
       "70156               V6J 1C7  \n",
       "81946                  2000  \n",
       "91356               V6J 1C7  \n",
       "93086               V6J 1C7  \n",
       "104514               V5T4T5  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_state_null_idx = np.where(h1b_df_clean14.EMPLOYER_STATE_MAPPED.isnull())[0]\n",
    "mapped_df = h1b_df_clean14.loc[mapped_state_null_idx,['EMPLOYER_NAME', 'EMPLOYER_STATE_MAPPED','EMPLOYER_POSTAL_CODE']]\n",
    "mapped_df.head(15)\n",
    "# mapped_df.EMPLOYER_NAME.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULT:** I only printed out the first 15 rows but we can begin to see a pattern here. First of all, some of the zipcodes are in strings, which are indicative of Canadian addresses. Second of all, there are only a handful of companies that have missing `STATE` values. (Quick view of `mapped_df.EMPLOYER_NAME.value_counts()` shows this.) A quick google search also reveals that **Lululemon USA Inc.** has its headquarters in Vancouver, Canada. This signifies that these rows with missing `STATE` columns are probably companies outside of United States. Thus, we will impute these values accordingly by replacing nan values with **\"Other\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Replacing nan values with \"Other\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1b_df_clean15 = h1b_df_clean14.copy()\n",
    "h1b_df_clean15.EMPLOYER_STATE_MAPPED = h1b_df_clean15.EMPLOYER_STATE_MAPPED.replace(np.nan,'Other')\n",
    "\n",
    "# # Checking to see that replacement occurred (dataframe)\n",
    "t_df = h1b_df_clean15.loc[mapped_state_null_idx,['EMPLOYER_NAME', 'EMPLOYER_STATE_MAPPED','EMPLOYER_POSTAL_CODE']]\n",
    "# t_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D. Check replacement worked by calling `display_missing_rows()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check missing values again \n",
    "# # Commented out for space constraints\n",
    "# display_missing_rows(h1b_df_clean15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#Table of Contents'>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Dataset Cleaning 2e'></a>\n",
    "____\n",
    "\n",
    "### 5e. Final Missing Value Handling\n",
    "\n",
    "As we saw above, there are now only a handful of missing values left. Since there are only a few rows with missing values left compared to the original dataset, it would just make sense to drop any rows that now contain any missing values. We will perform this operation and compare the final dataframe dimensions with that of the original dataframe. \n",
    "\n",
    "[How to Drop nans - Comment from WMcKinney himself!](https://stackoverflow.com/questions/13413590/how-to-drop-rows-of-pandas-dataframe-whose-value-in-a-certain-column-is-nan/13434501#13434501)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Dropping nan values from dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop missing values\n",
    "h1b_df_clean16 = h1b_df_clean15.dropna()\n",
    "h1b_df_clean_final = h1b_df_clean16.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Verification and Comparision with original dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compared with the original dataframe of dimensions (654360, 50), we are now left with a dimension of (654216, 32).\n",
      "Compared with the original dataframe, we are left with 99.9780% of rows.\n"
     ]
    }
   ],
   "source": [
    "# # Commented out due to space constraints\n",
    "# display_missing_rows(h1b_df_clean_final)\n",
    "\n",
    "# # Compare with Original Dataframe\n",
    "o_shape = h1b_df.shape\n",
    "f_shape = h1b_df_clean_final.shape\n",
    "\n",
    "print(\"Compared with the original dataframe of dimensions {}, we are now left with a dimension of {}.\".format(o_shape, f_shape))\n",
    "print(\"Compared with the original dataframe, we are left with {:0.4f}% of rows.\".format(f_shape[0]/o_shape[0] * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END OF NOTEBOOK\n",
    "<a href='#Top'>Back to Top</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
